<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Linear regression with Bayesian inference while imputing missing input data | Satoshi Ido</title> <meta name="author" content="Satoshi Ido"> <meta name="description" content="Statistical Machine Learning with Bayes Imputation in Python"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://idsts2670.github.io/blog/2024/linear-reg-with-Bayes-completion-of-missing-data/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Satoshi </span>Ido</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Linear regression with Bayesian inference while imputing missing input data</h1> <p class="post-meta">January 10, 2024</p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/category/sample-posts"> <i class="fa-solid fa-tag fa-sm"></i> sample-posts</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>予測モデルを構築する中で，データが欠損値を含むことは多々あります．素朴な対処方法としては①欠損値を含む行・列を捨てる，②平均値などの適当な値で埋める，の二つが考えられるでしょう．①・②いずれの場合も，情報を捨ててしまっていることになるため精度の低下につながります．いい方法がないか調べたところ，ベイズ的な考え方により補完が可能であると知りました． なお，今回の内容は以下の須山さんの記事を参考にしています．須山さんは2値分類をされているので，こちらではベイズ線形回帰をやってみることにしました．</p> <p>http://machine-learning.hatenablog.com/entry/2017/08/30/221801</p> <h1 id="欠損の分類と扱いやすさ">欠損の分類と扱いやすさ</h1> <p>あまり気にしたことはありませんでしたが，欠損にもいくつかの種類があるそうです．ここでは例として，性別と体重を説明変数とし，ある病気にかかっているか否かを予測するような問題を考えます．欠損の分類については初めて学んだので，誤解などあればご指摘いただけますと幸いです．</p> <ul> <li>MCAR (Missing Completely At Random): 欠損値の発生が完全にランダムである場合です．つまり，ほかの説明変数の値によって欠損が生じやすくなったりしないというケースです．例えば，何人かに対して体重の情報を聞き取るのをうっかり忘れてしまったとします．これはまさにMCARにあたります．</li> <li>MAR (Missing At Random): 欠損値の発生がほかの説明変数に依る場合です．先ほどの例では，性別が女性の場合に体重を申告してくれない場合が多かったとすると，これはMARに該当します．</li> <li>MNAR (Missing Not At Random): 他の説明変数を固定しても，欠損値の発生が欠損値の値に依る場合です．例としては，男女別で考えたとしても，体重が重い人は申告してくれないというケースが考えられます．他の説明変数を固定するというのが重要なポイントです．</li> </ul> <p>扱いやすさとしては，MCAR &gt; MAR &gt; MNARとなっています．MCARは以下で述べるベイズ的な考え方で対処することができます．MARに関しては，欠損の発生を表す確率変数を導入することで対処するようですが，わりと難しそうです．MNARへの対処も少し調べてみましたが，かなり厳しそうだということがわかりました． 今回の記事ではMCARの取り扱い方を説明します．</p> <h1 id="理論">理論</h1> <p>まず問題設定を明確にし，事後分布の直接計算が困難であることを示します．また他の変数を所与としたときの事後分布を解析的に計算し，ギブスサンプリングの手順を示します．</p> <h2 id="問題設定">問題設定</h2> <p>訓練データの入力全体を$X$で表し，その中で観測できたデータを$X_{\rm observed}$，欠損したデータを$X_{\rm missing}$と表すことにします．説明変数の数を$M$，データ数を$N$とすると，入力データ$X$は$N \times M$行列になっています．訓練データの出力値を${\bf y}$とします．また簡単のため，説明変数と目的変数はそれぞれ標準化(つまり平均0，分散1になるように)されているものとします． 今回はパラメータ${\bf w}$による線形回帰を扱うことにします：</p> \[{\bf y} = X {\bf w} + \hat{\bf \epsilon}\] <p>ただし$\hat{\bf \epsilon}$は平均$0$，分散$1/\beta$の正規分布に従うものとします．ここから，${\bf y}$の事後分布は</p> \[p({\bf y} \mid X, {\bf w}) = \left( \dfrac{\beta}{2\pi} \right)^{N/2} \exp \left[ - \dfrac{\beta}{2} ({\bf y} - X{\bf w})^2 \right]\] <p>となります． さらにパラメータ${\bf w}$と入力データ$X$の事前分布を以下のように仮定します：</p> \[p(\mathbf{w}) = \left( \frac{\alpha}{2\pi} \right)^{\frac{M}{2}} \exp \left[ -\frac{\alpha}{2} \mathbf{w}^T \mathbf{w} \right] \\ p(X) = \prod_{n} \left( \frac{ |\Lambda| }{2\pi} \right)^{\frac{1}{2}} \exp \left[ -\frac{1}{2} \mathbf{x}_n^T \Lambda \mathbf{x}_n \right]\] <p>ただし$\alpha$と$\Lambda$はそれぞれパラメータ${\bf w}$と入力$X$の分布を決めるパラメータです．</p> <p>いま考えたい問題はいくつかに分けることができるでしょう．</p> <ol> <li>与えられた訓練データ$X_{\rm observed}$と${\bf y}$から${\bf w}$と$X_{\rm missing}$の分布を推定する</li> <li>ハイパーパラメータ$\alpha$, $\beta$として適切な値を選ぶ($\Lambda$は$X_{\rm observed}$からうまく選べるはずなのでここでは無視)</li> <li>新しい入力${\bf x}$に対する出力値$y$を予測する 以下では1から3を実現する手続きを示します．</li> </ol> <h2 id="1-訓練データからパラメータの分布を推定">1. 訓練データからパラメータの分布を推定</h2> <p>ベイズの定理を用いると${\bf w}$と$X_{\rm missing}$の事後分布は</p> \[p(\mathbf{w}, X_{\text{missing}} \mid X_{\text{observed}}, \mathbf{y}) \propto p(\mathbf{y} \mid \mathbf{w}, X) p(\mathbf{w}) p(X_{\text{missing}} \mid X_{\text{observed}})\] <p>という形に分解することができます(各変数の依存関係に注意しつつ，丁寧に変形すると導けます)． 目的は事後分布$p({\bf w}, X_{\rm missing} \mid X_{\rm observed}, {\bf y})$の計算により${\bf w}$と$X_{\rm missing}$を推定することですが，この事後分布は簡単に取り扱うことができません．いつも通り${\bf w}$について平方完成しようとすると，その「おつり」の項が$X_{\rm missing}$を含んでおり，$X_{\rm missing}$について複雑な形状となっていしまうことが問題です． 一方で，他のすべての変数を所与としたときの事後分布は(少し大変な計算により)</p> \[\mathbf{w} \mid \sim \mathcal{N} \left( \left(X^\top X+ \frac{\alpha}{\beta} I_m \right)^{-1} X^\top \mathbf{y},~ \left(\beta X^\top X + \alpha I_m \right)^{-1} \right) \\ x_{n, m} \mid \sim \mathcal{N} \left( \frac{\beta y_n \mathbf{w}_m - \sum_{m' \neq m} (\beta \mathbf{w}_{m'} \mathbf{w}_m + \Lambda_{m, m'} ) x_{n,m'} }{\beta \mathbf{w}_m^2 + \Lambda_{m,m} },~ \frac{1 }{\beta \mathbf{w}_m^2 + \Lambda_{m,m}} \right)\] <p>となることが示せます． ゆえに以下に示す手続きから，事後分布$p({\bf w}, X_{\rm missing} \mid X_{\rm observed}, {\bf y})$に従う${\bf w}$と$X_{\rm missing}$を抽出することができます：</p> <p>(0) $X$を計算するために，平均値などを使い適当に欠損値を埋める． (1) ${\bf w} \mid *$の従う分布に基づき${\bf w}$をサンプリングする． (2) $x_{n, m} \mid *$の従う分布に基づき$x_{n, m}$をサンプリングする(ただし$(n,m)$は欠損値に対応するデータ数，説明変数のラベル)． (3) (1)(2)を繰り返す．</p> <p>これはまさにギブスサンプリングであり，サンプリングした結果の事後分布$p({\bf w}, X_{\rm missing} \mid X_{\rm observed}, {\bf y})$への収束が保証されている点が大きなメリットです．また，メトロポリス・ヘイスティング法では提案分布を決めるために新しいハイパーパラメータを導入する必要がありますが，ギブスサンプリングでは不要であるという点も便利です． ギブスサンプリングが可能だったのは，他の変数が所与であるとしたときの分布$p({\bf w} \mid *)$，$p(x_{n, m} \mid *)$が計算可能だったことに由来します．ここには今回のモデルが線形回帰であるという点が強く効いており，一般の場合にはギブスサンプリングは可能でないかもしれません．その場合は，メトロポリス・ヘイスティング法などを利用する必要があるでしょう．</p> <p>何にせよ，事後分布$p({\bf w}, X_{\rm missing} \mid X_{\rm observed}, {\bf y})$に従う${\bf w}$と$X_{\rm missing}$をサンプリングすることができるようになりました．ここから，事後分布$p({\bf w}, X_{\rm missing} \mid X_{\rm observed}, {\bf y})$を推定することができるようになります．実際には，${\bf w}, X_{\rm missing}$の平均や分散をもとめれば分布の特徴をおおよそ捉えることができるため，${\bf w}, X_{\rm missing} $のサンプル平均を計算することになります．</p> <h1 id="2-ハイパーパラメータの調整">2. ハイパーパラメータの調整</h1> <p>今回は最尤推定によってハイパーパラメータを調整しています．最尤推定とは$p({\bf y} \mid X_{\rm observed}, \alpha, \beta)$を最大化する$\alpha$, $\beta$を計算することですが，</p> \[\begin{align} p(\mathbf{y} \mid X_{\text{observed}}, \alpha, \beta) &amp;= \int p(\mathbf{y}, \mathbf{w}, X_{\text{missing}} \mid X_{\text{observed}}, \alpha, \beta)d\mathbf{w} dX_{\text{missing}} \\\\ &amp;= \int p(\mathbf{y} \mid \mathbf{w}, X, \alpha, \beta) p(\mathbf{w}, X_{\text{missing}} \mid X_{\text{observed}}, \alpha, \beta) d\mathbf{w} dX_{\text{missing}} \\\\ &amp;= \mathbb{E}_{\mathbf{w}, X_{\text{missing}} \mid X_{\text{observed}}, \alpha, \beta} \left[ \mathcal{N}(\mathbf{y} \mid X \mathbf{w}, \beta^{-1}) \right] \end{align}\] <p>によって$p({\bf y} \mid X_{\rm observed}, \alpha, \beta)$を計算できるため，これを最大化する$(\alpha, \beta)$を求めればよいことになります．事後分布$p({\bf w}, X_{\rm missing} ~\mid X_{\rm observed}~~, \alpha, \beta)$に基づく${\bf w}, X_{\rm missing}$をサンプリングすることができるため，以下の手続きで$\alpha$, $\beta$を調整できます：</p> <p>(0) $\alpha_0$, $\beta_0$を適当に初期化 (1) $L(\alpha_t, \beta_t) = \mathbb{E} _ {\bf w}, X _ {\rm missing} ~\mid X _ {\rm observed}~~, \alpha_t, \beta_t} \left[ \mathcal{N}({\bf y} \mid X {\bf w}, \beta_t) \right]$を，事後分布に基づきサンプリングされた${\bf w}, X_{\rm missing}$から計算 (2) 数値微分により$\alpha_{t+1} = \alpha_t + \eta \dfrac{\partial L}{\partial \alpha}(\alpha_t, \beta_t)$のように更新 ここで$\eta$は適当に決め打ちする必要があります．またスケールの問題から，実際には対数尤度を計算する方がよいでしょう．</p> <h1 id="3-新しい入力に対する予測">3. 新しい入力に対する予測</h1> <p>新しい入力${\bf x}$に対して予測をしたいものとします．今回は${\bf x}$が欠損している可能性があるため，欠損している${\bf x} _ {\rm missing}$と${\bf x} _ {\rm observed}$に分けます．事後分布$p({\bf x} _ {\rm missing}, y \mid {\bf x} _ {\rm observed}, X_{\rm observed}, {\bf y}, \alpha, \beta)$を解析的に計算することはできないので，ギブスサンプリングによって分布を推定します． 先ほどと同様に，他の変数を条件づけたときの分布は</p> \[\mathbf{w} \mid * \sim \mathcal{N} \left( \left(X^T X+ \frac{\alpha}{\beta} I_m \right)^{-1} X^T \mathbf{y},~ \left(\beta X^T X + \alpha I_m \right)^{-1} \right) \\\\ x_{m} \mid * \sim \mathcal{N} \left( \frac{\beta y w_m - \sum_{m' \neq m} (\beta w_{m'} w_m + \Lambda_{m, m'} ) x_{m'} }{\beta w_m^2 + \Lambda_{m,m} },~ \frac{1}{\beta w_m^2 + \Lambda_{m,m}} \right) \\\\ y \mid * \sim \mathcal{N}(\mathbf{w}^T \mathbf{x}, \beta^{-1})\] <p>となっているため，事後分布$p({\bf x} _ {\rm missing}, y , {\bf w} \mid {\bf x} _ {\rm observed}, X_{\rm observed}, {\bf y}, \alpha, \beta)$をサンプリングによって推定できます．ここで関係ないはずの${\bf w}$が混入していることに違和感があるかもしれませんが，${\bf w}$の事後分布がサンプルを通じてしか求められず，$y$の分布が${\bf w}$を通じて求められる以上，${\bf w}$も含めてサンプルする必要があります． いま，${\bf w}$は訓練データ$X, {\bf y}$のみに依ることに注意します．そのため，${\bf w}$は依然サンプリングした結果を保存してそのまま使うことができます．求めたいのは${\bf x}$と$y$の分布なので，サンプリングした結果から平均や分散を計算すればよいでしょう．</p> <h1 id="実装">実装</h1> <p>先ほどの理論に基づき，説明変数の補間が可能なベイズ線形回帰のクラス<code class="language-plaintext highlighter-rouge">Bayesian_LR_with_intepolation</code>を実装しました．このクラスの便利なところは，欠損に対する前処理をすることなくデータを入れてしまえばよいという点です．また標準化や事前分布の設定もクラス内でいい感じにやってくれます． 主なメソッドは以下のようになっています： <code class="language-plaintext highlighter-rouge">fit</code>：訓練データを標準化し，ハイパーパラメータを調整する．その後，事後分布$p({\bf w}, X_{\rm missing} \mid *)$に従う${\bf w}$と$X_{\rm missing}$をサンプリングする．サンプルの平均と標準偏差(つまり事後分布に基づくの期待値と標準偏差)を計算できる． <code class="language-plaintext highlighter-rouge">predict</code>：与えられたテストデータに対する予測をする．訓練データとテストデータを使いサンプリングし，出力の期待値と標準偏差を返す．テストデータに欠損がある場合は，欠損値に対する予測もする．</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Bayesian_LR_with_interpolation</span><span class="p">:</span> <span class="c1"># 説明変数の補間が可能な，ベイズ線形回帰
</span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">beta</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span> <span class="c1"># alphaはパラメータwの事前分布のパラメータで，alphaが大きいほどwの分布は狭くなる．
</span>        <span class="n">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">beta</span> <span class="c1"># betaは誤差のオーダーの逆数
</span>    
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X_train_original</span><span class="p">,</span> <span class="n">y_train_original</span><span class="p">,</span> <span class="n">num_sample</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">fit_hyper_parameters</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">num_iteration</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">eta</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">):</span>
        <span class="n">dalpha</span> <span class="o">=</span> <span class="mf">0.01</span>
        <span class="n">dbeta</span> <span class="o">=</span> <span class="mf">0.01</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hyper_param_record</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">num_iteration</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span> <span class="c1"># ハイパーパラメータの軌跡を保存
</span>        <span class="n">self</span><span class="p">.</span><span class="nf">normalize_train</span><span class="p">(</span><span class="n">X_train_original</span><span class="p">,</span> <span class="n">y_train_original</span><span class="p">)</span> <span class="c1"># 標準化されたself.X_train, self.y_trainが生成される
</span>        <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">nan_list</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Note: There are no missing values in the explanatory variables.</span><span class="sh">"</span><span class="p">)</span>
        
        <span class="n">w_sample_mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span> <span class="c1"># サンプリングから計算したwの期待値を格納
</span>        <span class="n">w_sample_std</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span> <span class="c1"># サンプリングから計算したwの標準偏差を格納
</span>        <span class="n">X_sample_mean</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">X_train</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span> <span class="c1"># サンプリングから計算したXの期待値を格納，欠損値だけ更新
</span>        <span class="n">X_sample_std</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">X_train</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># サンプリングから計算したXの標準偏差を格納，欠損値だけ更新
</span>        <span class="n">X_sample</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">X_train</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span> <span class="c1"># 現時点でのサンプルの値，欠損値だけ更新する
</span>        <span class="n">self</span><span class="p">.</span><span class="n">w_samples</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">num_sample</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">])))</span> <span class="c1"># wに関するサンプリング結果をすべて集める，予測の際に必要
</span>        
        <span class="c1"># ハイパーパラメータを調整，勾配法により尤度を上げるようなハイパーパラメータを探す
</span>        <span class="k">if</span> <span class="n">fit_hyper_parameters</span><span class="p">:</span> <span class="c1"># ハイパーパラメータを調整する場合
</span>            <span class="n">likelihood</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">likelihood</span><span class="p">(</span><span class="n">num_sample</span><span class="p">)</span> <span class="c1"># 初期化
</span>            <span class="k">for</span> <span class="n">idx_iteration</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_iteration</span><span class="p">):</span>
                <span class="n">self</span><span class="p">.</span><span class="n">hyper_param_record</span><span class="p">[</span><span class="n">idx_iteration</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">self</span><span class="p">.</span><span class="n">alpha</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">beta</span><span class="p">])</span>
                <span class="n">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">+</span> <span class="n">dalpha</span>
                <span class="n">likelihood_new</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">likelihood</span><span class="p">(</span><span class="n">num_sample</span><span class="p">)</span>
                <span class="n">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">-</span> <span class="n">dalpha</span> <span class="o">+</span> <span class="n">eta</span><span class="o">*</span><span class="p">(</span><span class="n">likelihood_new</span> <span class="o">-</span> <span class="n">likelihood</span><span class="p">)</span> <span class="o">/</span> <span class="n">dalpha</span>
                <span class="n">likelihood</span> <span class="o">=</span> <span class="n">likelihood_new</span>
                <span class="n">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">+</span> <span class="n">dbeta</span>
                <span class="n">likelihood_new</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">likelihood</span><span class="p">(</span><span class="n">num_sample</span><span class="p">)</span> <span class="c1"># 新しい尤度を計算
</span>                <span class="n">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">-</span> <span class="n">dbeta</span> <span class="o">+</span> <span class="n">eta</span><span class="o">*</span><span class="p">(</span><span class="n">likelihood_new</span> <span class="o">-</span> <span class="n">likelihood</span><span class="p">)</span> <span class="o">/</span> <span class="n">dbeta</span>
                <span class="n">likelihood</span> <span class="o">=</span> <span class="n">likelihood_new</span>
                <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">iteraton = {0:} / {1:}: alpha = {2:.6f}, beta = {3:.6f}, L = {4:.6f}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">idx_iteration</span><span class="p">,</span> <span class="n">num_iteration</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">alpha</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">beta</span><span class="p">,</span> <span class="n">likelihood</span><span class="p">),</span> <span class="n">end</span> <span class="o">=</span> <span class="sh">"</span><span class="se">\r</span><span class="sh">"</span><span class="p">)</span>
        
        <span class="c1"># サンプリングによりwとXの統計量を計算
</span>        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_sample</span><span class="p">):</span>
            <span class="c1"># wのサンプリング
</span>            <span class="n">w_sample</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">sample_w</span><span class="p">(</span><span class="n">X_sample</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">w_samples</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">w_sample</span> <span class="c1"># predictionで使うため
</span>            <span class="n">w_sample_mean</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">update_mean</span><span class="p">(</span><span class="n">w_sample_mean</span><span class="p">,</span> <span class="n">w_sample</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
            <span class="n">w_sample_std</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">update_std</span><span class="p">(</span><span class="n">w_sample_std</span><span class="p">,</span> <span class="n">w_sample</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
            
            <span class="c1"># Xのサンプリング(欠損している箇所のみ)
</span>            <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">nan_list</span><span class="p">):</span>
                <span class="n">X_sample</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">sample_x_nm</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">w_sample</span><span class="p">,</span> <span class="n">X_sample</span><span class="p">[</span><span class="n">n</span><span class="p">],</span> <span class="n">self</span><span class="p">.</span><span class="n">y_train</span><span class="p">[</span><span class="n">n</span><span class="p">])</span>
                <span class="n">X_sample_mean</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">update_mean</span><span class="p">(</span><span class="n">X_sample_mean</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">],</span> <span class="n">X_sample</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">],</span> <span class="n">t</span><span class="p">)</span>
                <span class="n">X_sample_std</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">update_std</span><span class="p">(</span><span class="n">X_sample_std</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">],</span> <span class="n">X_sample</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">],</span> <span class="n">t</span><span class="p">)</span>
                
        <span class="c1"># 標準化を解除
</span>        <span class="n">X_sample_mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">tile</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">x_train_original_mean</span><span class="p">,</span> <span class="n">reps</span> <span class="o">=</span> <span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">X_train_original</span><span class="p">),</span> <span class="mi">1</span><span class="p">))</span> \
                            <span class="o">+</span> <span class="n">X_sample_mean</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">tile</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">x_train_original_std</span><span class="p">,</span> <span class="n">reps</span> <span class="o">=</span> <span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">X_train_original</span><span class="p">),</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">X_sample_std</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">tile</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">x_train_original_std</span><span class="p">,</span> <span class="n">reps</span> <span class="o">=</span> <span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">X_train_original</span><span class="p">),</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">X_sample_std</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Fitting has finished: </span><span class="sh">"</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">alpha = {:.6f}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">alpha</span><span class="p">))</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">beta = {:.6f}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">beta</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">w_sample_mean</span><span class="p">,</span> <span class="n">w_sample_std</span><span class="p">,</span> <span class="n">X_sample_mean</span><span class="p">,</span> <span class="n">X_sample_std</span>
    
    <span class="k">def</span> <span class="nf">normalize_train</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X_train_original</span><span class="p">,</span> <span class="n">y_train_original</span><span class="p">):</span> <span class="c1"># 入力を標準化しつつ，標準化を解除できるように値を保存
</span>        <span class="c1"># Xについての処理
</span>        <span class="n">self</span><span class="p">.</span><span class="n">x_train_original_mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">nanmean</span><span class="p">(</span><span class="n">X_train_original</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">x_train_original_std</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">nanstd</span><span class="p">(</span><span class="n">X_train_original</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">X_train</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_train_original</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nf">tile</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">x_train_original_mean</span><span class="p">,</span> <span class="n">reps</span> <span class="o">=</span> <span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">X_train_original</span><span class="p">),</span><span class="mi">1</span><span class="p">))</span> <span class="p">)</span> \
                            <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">tile</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">x_train_original_std</span><span class="p">,</span> <span class="n">reps</span> <span class="o">=</span> <span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">X_train_original</span><span class="p">),</span> <span class="mi">1</span><span class="p">))</span> <span class="c1"># 標準化
</span>        <span class="n">self</span><span class="p">.</span><span class="n">Lambda</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">inv</span><span class="p">(</span> <span class="n">np</span><span class="p">.</span><span class="nf">cov</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">X_train</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">isnan</span><span class="p">(</span><span class="n">X_train_original</span><span class="p">).</span><span class="nf">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">].</span><span class="n">T</span><span class="p">)</span> <span class="p">)</span><span class="c1"># 説明変数がどれも欠損していないデータ行から計算した共分散行列の逆行列
</span>        <span class="n">self</span><span class="p">.</span><span class="n">nan_list</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">isnan</span><span class="p">(</span><span class="n">X_train_original</span><span class="p">))))</span> <span class="c1"># もとの入力データでnullの場所を格納
</span>        <span class="n">self</span><span class="p">.</span><span class="n">X_train</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">isnan</span><span class="p">(</span><span class="n">X_train_original</span><span class="p">)]</span> <span class="o">=</span> <span class="mf">0.0</span> <span class="c1"># 標準化しているため0で埋めてok
</span>        
        <span class="c1"># yについての処理
</span>        <span class="n">self</span><span class="p">.</span><span class="n">y_train_original_mean</span> <span class="o">=</span> <span class="n">y_train_original</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">y_train_original_std</span> <span class="o">=</span> <span class="n">y_train_original</span><span class="p">.</span><span class="nf">std</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">y_train</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_train_original</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">y_train_original_mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">y_train_original_std</span>
    
    <span class="k">def</span> <span class="nf">likelihood</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">num_sample</span><span class="p">):</span>
        <span class="n">L</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="n">X_sample</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">X_train</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_sample</span><span class="p">):</span>
            <span class="n">w_sample</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">sample_w</span><span class="p">(</span><span class="n">X_sample</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">nan_list</span><span class="p">):</span>
                <span class="n">X_sample</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">sample_x_nm</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">w_sample</span><span class="p">,</span> <span class="n">X_sample</span><span class="p">[</span><span class="n">n</span><span class="p">],</span> <span class="n">self</span><span class="p">.</span><span class="n">y_train</span><span class="p">[</span><span class="n">n</span><span class="p">])</span>
            <span class="n">L</span> <span class="o">+=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">*</span> <span class="p">((</span><span class="n">self</span><span class="p">.</span><span class="n">y_train</span> <span class="o">-</span> <span class="n">X_sample</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">w_sample</span><span class="p">))</span><span class="o">**</span><span class="mf">2.0</span><span class="p">).</span><span class="nf">sum</span><span class="p">()</span> <span class="p">)</span>
        <span class="c1"># 訓練データ数が増えるにつれてlog_likelihoodは線形に増える(はず)なので規格化．これをしないとデータ数を変えるたびにetaを変更する必要がありそう
</span>        <span class="n">L</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">L</span><span class="p">)</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">y_train</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">beta</span><span class="p">)</span> 
        <span class="k">return</span> <span class="n">L</span>
    
    <span class="k">def</span> <span class="nf">update_mean</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x_mean</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="nf">return </span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">t</span><span class="o">+</span><span class="mf">1.0</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span><span class="n">t</span><span class="o">*</span><span class="n">x_mean</span> <span class="o">+</span> <span class="n">x</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">update_std</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x_std</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span> <span class="p">(</span><span class="n">t</span><span class="o">/</span><span class="p">(</span><span class="n">t</span><span class="o">+</span><span class="mf">1.0</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span><span class="n">x_std</span><span class="o">**</span><span class="mf">2.0</span> <span class="o">+</span> <span class="p">(</span><span class="n">x_std</span><span class="o">-</span><span class="n">x</span><span class="p">)</span><span class="o">**</span><span class="mf">2.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">t</span><span class="o">+</span><span class="mf">1.0</span><span class="p">)</span> <span class="p">)</span> <span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">sample_w</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span> <span class="c1"># 他を条件づけたときのwをサンプル
</span>        <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">y_train</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">alpha</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">beta</span>
        <span class="n">w_mu</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">inv</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">alpha</span><span class="o">/</span><span class="n">beta</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">]))).</span><span class="nf">dot</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">T</span><span class="p">).</span><span class="nf">dot</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="c1"># 他の変数を条件づけたときの分布の期待値n
</span>        <span class="n">w_sigma</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">inv</span><span class="p">(</span><span class="n">beta</span> <span class="o">*</span> <span class="n">X</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">alpha</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">])))</span> <span class="c1"># 標準偏差
</span>        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">multivariate_normal</span><span class="p">(</span><span class="n">w_mu</span><span class="p">,</span> <span class="n">w_sigma</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">sample_x_nm</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">x_n</span><span class="p">,</span> <span class="n">y_n</span><span class="p">):</span> <span class="c1"># 他を条件づけたときのx_nm(欠損値)をサンプル
</span>        <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">alpha</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">beta</span>
        <span class="n">x_nm_mu</span> <span class="o">=</span> <span class="n">x_n</span><span class="p">[</span><span class="n">m</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">beta</span><span class="o">*</span><span class="n">y_n</span><span class="o">*</span><span class="n">w</span><span class="p">[</span><span class="n">m</span><span class="p">]</span> <span class="o">-</span> <span class="p">(</span><span class="n">beta</span><span class="o">*</span><span class="n">w</span><span class="o">*</span><span class="n">w</span><span class="p">[</span><span class="n">m</span><span class="p">]</span><span class="o">+</span><span class="n">self</span><span class="p">.</span><span class="n">Lambda</span><span class="p">[</span><span class="n">m</span><span class="p">]).</span><span class="nf">dot</span><span class="p">(</span><span class="n">x_n</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">beta</span><span class="o">*</span><span class="n">w</span><span class="p">[</span><span class="n">m</span><span class="p">]</span><span class="o">*</span><span class="n">w</span><span class="p">[</span><span class="n">m</span><span class="p">]</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">Lambda</span><span class="p">[</span><span class="n">m</span><span class="p">,</span><span class="n">m</span><span class="p">])</span>
        <span class="n">x_nm_sigma</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">beta</span><span class="o">*</span><span class="n">w</span><span class="p">[</span><span class="n">m</span><span class="p">]</span><span class="o">*</span><span class="n">w</span><span class="p">[</span><span class="n">m</span><span class="p">]</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">Lambda</span><span class="p">[</span><span class="n">m</span><span class="p">,</span><span class="n">m</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">x_nm_mu</span><span class="p">,</span> <span class="n">x_nm_sigma</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">sample_y</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span> <span class="c1"># 他を条件づけたときの出力yをサンプル
</span>        <span class="n">y_mu</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
        <span class="n">y_sigma</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span><span class="o">/</span><span class="n">self</span><span class="p">.</span><span class="n">beta</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">y_mu</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">multivariate_normal</span><span class="p">(</span><span class="n">y_mu</span><span class="p">,</span> <span class="n">y_sigma</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X_test_original</span><span class="p">,</span> <span class="n">num_sample</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">):</span> <span class="c1"># 欠損がある可能性のある入力から予測
</span>        <span class="n">X_test</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_test_original</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nf">tile</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">x_train_original_mean</span><span class="p">,</span> <span class="n">reps</span> <span class="o">=</span> <span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">X_test_original</span><span class="p">),</span><span class="mi">1</span><span class="p">))</span> <span class="p">)</span> \
                            <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">tile</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">x_train_original_std</span><span class="p">,</span> <span class="n">reps</span> <span class="o">=</span> <span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">X_test_original</span><span class="p">),</span> <span class="mi">1</span><span class="p">))</span> <span class="c1"># 標準化
</span>        <span class="n">nan_list</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">isnan</span><span class="p">(</span><span class="n">X_test_original</span><span class="p">))))</span> <span class="c1"># もとの入力データでnullの場所を格納
</span>        <span class="n">X_test</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">isnan</span><span class="p">(</span><span class="n">X_test_original</span><span class="p">)]</span> <span class="o">=</span> <span class="mf">0.0</span> <span class="c1"># 標準化しているため0で埋めてok
</span>        
        <span class="n">X_test_sample_mean</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span> <span class="c1"># サンプリングから計算したXの期待値を格納，欠損値だけ更新
</span>        <span class="n">X_test_sample_std</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">X_test</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># サンプリングから計算したXの標準偏差を格納，欠損値だけ更新
</span>        <span class="n">X_test_sample</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span> <span class="c1"># 現時点でのサンプルの値，欠損値だけ更新する
</span>        
        <span class="n">y_test_sample_mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">X_test_original</span><span class="p">))</span>
        <span class="n">y_test_sample_std</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">X_test_original</span><span class="p">))</span>
        
        <span class="c1"># サンプリングによりyとXの統計量を計算
</span>        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_sample</span><span class="p">):</span>
            <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">prediction: t = {0:} / {1:}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">num_sample</span><span class="p">),</span> <span class="n">end</span> <span class="o">=</span> <span class="sh">"</span><span class="se">\r</span><span class="sh">"</span><span class="p">)</span>
            <span class="c1"># wのサンプル(前の結果を流用)
</span>            <span class="n">w_sample</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">w_samples</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
            <span class="c1"># yのサンプル
</span>            <span class="n">y_test_sample</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">sample_y</span><span class="p">(</span><span class="n">X_test_sample</span><span class="p">,</span> <span class="n">w_sample</span><span class="p">)</span>
            <span class="n">y_test_sample_mean</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">update_mean</span><span class="p">(</span><span class="n">y_test_sample_mean</span><span class="p">,</span> <span class="n">y_test_sample</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
            <span class="n">y_test_sample_std</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">update_std</span><span class="p">(</span><span class="n">y_test_sample_std</span><span class="p">,</span> <span class="n">y_test_sample</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
            <span class="c1"># Xのサンプル           
</span>            <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">nan_list</span><span class="p">):</span>
                <span class="n">X_test_sample</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">sample_x_nm</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">w_sample</span><span class="p">,</span> <span class="n">X_test_sample</span><span class="p">[</span><span class="n">n</span><span class="p">],</span> <span class="n">y_test_sample</span><span class="p">[</span><span class="n">n</span><span class="p">])</span>
                <span class="n">X_test_sample_mean</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">update_mean</span><span class="p">(</span><span class="n">X_test_sample_mean</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">],</span> <span class="n">X_test_sample</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">],</span> <span class="n">t</span><span class="p">)</span>
                <span class="n">X_test_sample_std</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">update_std</span><span class="p">(</span><span class="n">X_test_sample_std</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">],</span> <span class="n">X_test_sample</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">],</span> <span class="n">t</span><span class="p">)</span>
        <span class="c1"># 標準化を解除
</span>        <span class="n">y_test_sample_mean</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">y_train_original_mean</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">y_train_original_std</span> <span class="o">*</span> <span class="n">y_test_sample_mean</span>
        <span class="n">y_test_sample_std</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">y_train_original_std</span> <span class="o">*</span> <span class="n">y_test_sample_std</span>
        <span class="n">X_test_sample_mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">tile</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">x_train_original_mean</span><span class="p">,</span> <span class="n">reps</span> <span class="o">=</span> <span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span> <span class="mi">1</span><span class="p">))</span> \
                            <span class="o">+</span> <span class="n">X_test_sample_mean</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">tile</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">x_train_original_std</span><span class="p">,</span> <span class="n">reps</span> <span class="o">=</span> <span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">X_test_sample_std</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">tile</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">x_train_original_std</span><span class="p">,</span> <span class="n">reps</span> <span class="o">=</span> <span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">X_test_sample_std</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Predictions has finished.</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y_test_sample_mean</span><span class="p">,</span> <span class="n">y_test_sample_std</span><span class="p">,</span> <span class="n">X_test_sample_mean</span><span class="p">,</span> <span class="n">X_test_sample_std</span>
    
</code></pre></div></div> <h1 id="数値実験">数値実験</h1> <p>実装がうまくいっているか確認するために，トイモデルを作成してみました．実装は以下のリンクにあります． https://github.com/yotapoon/Bayesian_LR_with_interpolation</p> <h2 id="設定">設定</h2> <p>説明変数は$(x_1, x_2)$とし，目的変数を$y$とします．これらは</p> <pre><code class="language-math">x_1 \sim \mathcal{N}(2.0, 0.4) \\
x_2 \mid x_1 \sim \mathcal{N}(-2.0 x_1 + 0.5, 0.8) \\
y \mid x_1,x_2 \sim \mathcal{N}(3.0 x_1 + 1.5x_2, 0.8)
</code></pre> <p>のような関係にあるものとし，訓練データとして$N = 100$点を生成します． ここではMCARの状況を考えます．つまり，欠損の発生が他の変数に依存しないような扱いやすい状況です．また訓練データの入力値$X$の各要素には確率$p = 0.3$で欠損が生じるものとします．$X$は以下のような形状です：</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[ 1.58535427         nan]
 [ 2.10601671 -4.55318535]
 [ 1.80926526 -3.5396434 ]
...
</code></pre></div></div> <p>上のようなデータに対して，以下の四つの方法により<code class="language-plaintext highlighter-rouge">fit</code>します： ① 欠損のないデータ(complete data)を用いて回帰． ② $x_2$をその平均値により補完． ③ $x_2$が欠損している行をそのまま削除． ④ ベイズ線形回帰により補完．</p> <p>また同様のテストデータを$N’ = 1000$点生成し，テスト誤差を計算することでそれぞれの性能を評価します．ただし，各手法を比較しやすくするためにテストデータに欠損はないものとします． 期待される結果は以下の通りです： ・欠損のないデータによる回帰①は最も良い性能を与えてほしい． ・平均値による補完②はあまりよくないと聞くので，それほど精度がでないはず ・欠損している行をそのまま削除する方法③は情報量が減るためそこまでいい結果にはならないはず ・ベイズ線形回帰④をすると欠損のないデータによる回帰①より精度は出ないはずだが，適当な補完②や行の削除③よりは精度は出てほしい</p> <h2 id="結果と考察">結果と考察</h2> <p>数値実験の結果を以下に示します．確認のため，今回実装したクラスを使わない最尤推定の結果も示しています．<code class="language-plaintext highlighter-rouge">y_error</code>は$y$に関するテスト誤差の平均値，<code class="language-plaintext highlighter-rouge">X_error</code>は$X$に関する訓練誤差の平均値です．</p> <p>① 欠損のないデータ(complete data)を用いて回帰．</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>=====最尤推定=====
y_error = 0.6841894767175021

=====ベイズ線形回帰=====
Note: There are no missing values in the explanatory variables.
alpha = 1.482402
beta = 2.115292
y_error = 0.6814939439707199
</code></pre></div></div> <p>② $x_2$をその平均値により補完．</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>=====最尤推定=====
y_error = 0.9199739418129449

=====ベイズ線形回帰=====
Note: There are no missing values in the explanatory variables.
alpha = 4.767952
beta = 1.785241
X_error = 0.03779597748274369, 0.5054058111998435
y_error = 0.9434735826603966
</code></pre></div></div> <p>③ $x_2$が欠損している行をそのまま削除．</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>=====最尤推定=====
y_error = 0.7259293059445424

=====ベイズ線形回帰=====
Note: There are no missing values in the explanatory variables.
alpha = 1.721914
beta = 3.476663
y_error = 0.7766452769398361
</code></pre></div></div> <p>④ ベイズ線形回帰により補完．</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>=====ベイズ線形回帰=====
alpha = 2.567708
beta = 4.346280
X_error = 0.024276115858132833, 0.2436076829842362
y_error = 0.7147843731641442
</code></pre></div></div> <p>最尤推定とベイズ線形回帰の$y$に関する誤差が割と整合しているので，結果は正しそうです． また<code class="language-plaintext highlighter-rouge">y_error</code>を比較すると，平均で補完 &gt; そのまま削除 &gt; ベイズ線形回帰 &gt; complete dataとなっているため，期待された結果が得られています．さらに<code class="language-plaintext highlighter-rouge">X_error</code>をみると，ベイズ線形回帰は平均値による補完と比較して優れていることがわかります． 今回は欠損の生じていないテストデータに対する予測により評価しましたが，本手法は欠損が生じているデータに対してもある程度の精度で予測が可能です．これは他の手法と比べると大きなアドバンテージです．</p> <p>一方で，生成したデータによってはベイズ線形回帰の優位性がそれほど明らかでなかった場合がありました．特にデータ数が多かったりノイズが小さかったりするときには，欠損している行を削除してしまってもよい結果が得られます．そのため，本手法が効果を発揮するのは「入出力がある程度の相関を持ちつつ，ノイズも大きいような場合」であるといえそうです．</p> <h1 id="まとめ">まとめ</h1> <p>今回は欠損のある入力をベイズ的に補完しつつ，欠損値の推定，ハイパーパラメータの調整，新しい入力に対する予測を行うクラスを実装しました．数値実験から，実装が正しそうであることとベイズ線形回帰の効果を確認しました．本手法のメリットとしては， ・情報量増加による予測精度の向上 ・訓練データへの前処理の自動化 ・欠損のあるデータに対する予測能力 があります．特に最後の点に関しては，その他の手法に比べ非常に大きな利点であるといえます．</p> <p>一方で，デメリットとしては ・導出，実装が非常に面倒であること ・サンプリングのために計算量が増加すること が挙げられます．前者に関しては，欠損値を埋めたいという強いモチベーションがない限り，割に合わないというのが正直な感想です(導出・実装するのに丸二日かかった)．後者に関しては，事後分布を解析的に書けないためどうしても避けられないと考えます(もしかしたらできるのかもしれませんが．．．たかが線形回帰と思っていましたが，意外と複雑になるようです)．</p> <p>今回は最も扱いやすいMCARにフォーカスしましたが，MARの場合は欠損が生じる確率をモデル化することにより同様の解析が可能になります．きちんとは読んでいませんが， https://bookdown.org/marklhc/notes_bookdown/missing-data.html がそのような内容になっていると思います．時間があれば，MARの場合にも同様の実装をするつもりです．</p> <p>また，平均値による補完と欠損値の削除がそれぞれ有利になるのはどのような状況なのかが少し気になりました．データ数やノイズの大きさ，変数間の相関に依存すると思いますが，理論的な解析もできるのかもしれません．</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/sidebar-table-of-contents/">a post with table of contents on a sidebar</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/jupyter-notebook/">a post with jupyter notebook</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/custom-blockquotes/">a post with custom blockquotes</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/name-matching-pharma/">Research on M&amp;A in Pharmaceutical industry - Name Matching Codes</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/AMEX-competition/">Kaggle AMEX competition EDA sample</a> </li> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Satoshi Ido. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://github.com/idsts2670" target="_blank" rel="external nofollow noopener">Satoshi's GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>