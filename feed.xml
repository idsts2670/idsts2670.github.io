<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://idsts2670.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://idsts2670.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-05-16T14:17:52-04:00</updated><id>https://idsts2670.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Kaggle AMEX competition EDA sample</title><link href="https://idsts2670.github.io/blog/2024/AMEX-competition/" rel="alternate" type="text/html" title="Kaggle AMEX competition EDA sample"/><published>2024-01-15T23:00:00-05:00</published><updated>2024-01-15T23:00:00-05:00</updated><id>https://idsts2670.github.io/blog/2024/AMEX-competition</id><content type="html" xml:base="https://idsts2670.github.io/blog/2024/AMEX-competition/"><![CDATA[<div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/amex-eda.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div>]]></content><author><name></name></author><category term="sample-posts"/><category term="jupyter"/><summary type="html"><![CDATA[Introduction of Amex competition EDA jupyternotebook]]></summary></entry><entry><title type="html">linear-reg-baye-imputation</title><link href="https://idsts2670.github.io/blog/2024/linear-reg-baye-imputation/" rel="alternate" type="text/html" title="linear-reg-baye-imputation"/><published>2024-01-10T22:06:48-05:00</published><updated>2024-01-10T22:06:48-05:00</updated><id>https://idsts2670.github.io/blog/2024/linear-reg-baye-imputation</id><content type="html" xml:base="https://idsts2670.github.io/blog/2024/linear-reg-baye-imputation/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">Linear regression with Bayesian inference while imputing missing input data</title><link href="https://idsts2670.github.io/blog/2024/linear-reg-with-Bayes-completion-of-missing-data/" rel="alternate" type="text/html" title="Linear regression with Bayesian inference while imputing missing input data"/><published>2024-01-10T08:56:00-05:00</published><updated>2024-01-10T08:56:00-05:00</updated><id>https://idsts2670.github.io/blog/2024/linear-reg-with-Bayes-completion-of-missing-data</id><content type="html" xml:base="https://idsts2670.github.io/blog/2024/linear-reg-with-Bayes-completion-of-missing-data/"><![CDATA[<p>予測モデルを構築する中で，データが欠損値を含むことは多々あります．素朴な対処方法としては①欠損値を含む行・列を捨てる，②平均値などの適当な値で埋める，の二つが考えられるでしょう．①・②いずれの場合も，情報を捨ててしまっていることになるため精度の低下につながります．いい方法がないか調べたところ，ベイズ的な考え方により補完が可能であると知りました． なお，今回の内容は以下の須山さんの記事を参考にしています．須山さんは2値分類をされているので，こちらではベイズ線形回帰をやってみることにしました．</p> <p>http://machine-learning.hatenablog.com/entry/2017/08/30/221801</p> <h1 id="欠損の分類と扱いやすさ">欠損の分類と扱いやすさ</h1> <p>あまり気にしたことはありませんでしたが，欠損にもいくつかの種類があるそうです．ここでは例として，性別と体重を説明変数とし，ある病気にかかっているか否かを予測するような問題を考えます．欠損の分類については初めて学んだので，誤解などあればご指摘いただけますと幸いです．</p> <ul> <li>MCAR (Missing Completely At Random): 欠損値の発生が完全にランダムである場合です．つまり，ほかの説明変数の値によって欠損が生じやすくなったりしないというケースです．例えば，何人かに対して体重の情報を聞き取るのをうっかり忘れてしまったとします．これはまさにMCARにあたります．</li> <li>MAR (Missing At Random): 欠損値の発生がほかの説明変数に依る場合です．先ほどの例では，性別が女性の場合に体重を申告してくれない場合が多かったとすると，これはMARに該当します．</li> <li>MNAR (Missing Not At Random): 他の説明変数を固定しても，欠損値の発生が欠損値の値に依る場合です．例としては，男女別で考えたとしても，体重が重い人は申告してくれないというケースが考えられます．他の説明変数を固定するというのが重要なポイントです．</li> </ul> <p>扱いやすさとしては，MCAR &gt; MAR &gt; MNARとなっています．MCARは以下で述べるベイズ的な考え方で対処することができます．MARに関しては，欠損の発生を表す確率変数を導入することで対処するようですが，わりと難しそうです．MNARへの対処も少し調べてみましたが，かなり厳しそうだということがわかりました． 今回の記事ではMCARの取り扱い方を説明します．</p> <h1 id="理論">理論</h1> <p>まず問題設定を明確にし，事後分布の直接計算が困難であることを示します．また他の変数を所与としたときの事後分布を解析的に計算し，ギブスサンプリングの手順を示します．</p> <h2 id="問題設定">問題設定</h2> <p>訓練データの入力全体を$X$で表し，その中で観測できたデータを$X_{\rm observed}$，欠損したデータを$X_{\rm missing}$と表すことにします．説明変数の数を$M$，データ数を$N$とすると，入力データ$X$は$N \times M$行列になっています．訓練データの出力値を${\bf y}$とします．また簡単のため，説明変数と目的変数はそれぞれ標準化(つまり平均0，分散1になるように)されているものとします． 今回はパラメータ${\bf w}$による線形回帰を扱うことにします：</p> \[{\bf y} = X {\bf w} + \hat{\bf \epsilon}\] <p>ただし$\hat{\bf \epsilon}$は平均$0$，分散$1/\beta$の正規分布に従うものとします．ここから，${\bf y}$の事後分布は</p> \[p({\bf y} \mid X, {\bf w}) = \left( \dfrac{\beta}{2\pi} \right)^{N/2} \exp \left[ - \dfrac{\beta}{2} ({\bf y} - X{\bf w})^2 \right]\] <p>となります． さらにパラメータ${\bf w}$と入力データ$X$の事前分布を以下のように仮定します：</p> \[p(\mathbf{w}) = \left( \frac{\alpha}{2\pi} \right)^{\frac{M}{2}} \exp \left[ -\frac{\alpha}{2} \mathbf{w}^T \mathbf{w} \right] \\ p(X) = \prod_{n} \left( \frac{ |\Lambda| }{2\pi} \right)^{\frac{1}{2}} \exp \left[ -\frac{1}{2} \mathbf{x}_n^T \Lambda \mathbf{x}_n \right]\] <p>ただし$\alpha$と$\Lambda$はそれぞれパラメータ${\bf w}$と入力$X$の分布を決めるパラメータです．</p> <p>いま考えたい問題はいくつかに分けることができるでしょう．</p> <ol> <li>与えられた訓練データ$X_{\rm observed}$と${\bf y}$から${\bf w}$と$X_{\rm missing}$の分布を推定する</li> <li>ハイパーパラメータ$\alpha$, $\beta$として適切な値を選ぶ($\Lambda$は$X_{\rm observed}$からうまく選べるはずなのでここでは無視)</li> <li>新しい入力${\bf x}$に対する出力値$y$を予測する 以下では1から3を実現する手続きを示します．</li> </ol> <h2 id="1-訓練データからパラメータの分布を推定">1. 訓練データからパラメータの分布を推定</h2> <p>ベイズの定理を用いると${\bf w}$と$X_{\rm missing}$の事後分布は</p> \[p(\mathbf{w}, X_{\text{missing}} \mid X_{\text{observed}}, \mathbf{y}) \propto p(\mathbf{y} \mid \mathbf{w}, X) p(\mathbf{w}) p(X_{\text{missing}} \mid X_{\text{observed}})\] <p>という形に分解することができます(各変数の依存関係に注意しつつ，丁寧に変形すると導けます)． 目的は事後分布$p({\bf w}, X_{\rm missing} \mid X_{\rm observed}, {\bf y})$の計算により${\bf w}$と$X_{\rm missing}$を推定することですが，この事後分布は簡単に取り扱うことができません．いつも通り${\bf w}$について平方完成しようとすると，その「おつり」の項が$X_{\rm missing}$を含んでおり，$X_{\rm missing}$について複雑な形状となっていしまうことが問題です． 一方で，他のすべての変数を所与としたときの事後分布は(少し大変な計算により)</p> \[\mathbf{w} \mid \sim \mathcal{N} \left( \left(X^\top X+ \frac{\alpha}{\beta} I_m \right)^{-1} X^\top \mathbf{y},~ \left(\beta X^\top X + \alpha I_m \right)^{-1} \right) \\ x_{n, m} \mid \sim \mathcal{N} \left( \frac{\beta y_n \mathbf{w}_m - \sum_{m' \neq m} (\beta \mathbf{w}_{m'} \mathbf{w}_m + \Lambda_{m, m'} ) x_{n,m'} }{\beta \mathbf{w}_m^2 + \Lambda_{m,m} },~ \frac{1 }{\beta \mathbf{w}_m^2 + \Lambda_{m,m}} \right)\] <p>となることが示せます． ゆえに以下に示す手続きから，事後分布$p({\bf w}, X_{\rm missing} \mid X_{\rm observed}, {\bf y})$に従う${\bf w}$と$X_{\rm missing}$を抽出することができます：</p> <p>(0) $X$を計算するために，平均値などを使い適当に欠損値を埋める． (1) ${\bf w} \mid *$の従う分布に基づき${\bf w}$をサンプリングする． (2) $x_{n, m} \mid *$の従う分布に基づき$x_{n, m}$をサンプリングする(ただし$(n,m)$は欠損値に対応するデータ数，説明変数のラベル)． (3) (1)(2)を繰り返す．</p> <p>これはまさにギブスサンプリングであり，サンプリングした結果の事後分布$p({\bf w}, X_{\rm missing} \mid X_{\rm observed}, {\bf y})$への収束が保証されている点が大きなメリットです．また，メトロポリス・ヘイスティング法では提案分布を決めるために新しいハイパーパラメータを導入する必要がありますが，ギブスサンプリングでは不要であるという点も便利です． ギブスサンプリングが可能だったのは，他の変数が所与であるとしたときの分布$p({\bf w} \mid *)$，$p(x_{n, m} \mid *)$が計算可能だったことに由来します．ここには今回のモデルが線形回帰であるという点が強く効いており，一般の場合にはギブスサンプリングは可能でないかもしれません．その場合は，メトロポリス・ヘイスティング法などを利用する必要があるでしょう．</p> <p>何にせよ，事後分布$p({\bf w}, X_{\rm missing} \mid X_{\rm observed}, {\bf y})$に従う${\bf w}$と$X_{\rm missing}$をサンプリングすることができるようになりました．ここから，事後分布$p({\bf w}, X_{\rm missing} \mid X_{\rm observed}, {\bf y})$を推定することができるようになります．実際には，${\bf w}, X_{\rm missing}$の平均や分散をもとめれば分布の特徴をおおよそ捉えることができるため，${\bf w}, X_{\rm missing} $のサンプル平均を計算することになります．</p> <h1 id="2-ハイパーパラメータの調整">2. ハイパーパラメータの調整</h1> <p>今回は最尤推定によってハイパーパラメータを調整しています．最尤推定とは$p({\bf y} \mid X_{\rm observed}, \alpha, \beta)$を最大化する$\alpha$, $\beta$を計算することですが，</p> \[\begin{align} p(\mathbf{y} \mid X_{\text{observed}}, \alpha, \beta) &amp;= \int p(\mathbf{y}, \mathbf{w}, X_{\text{missing}} \mid X_{\text{observed}}, \alpha, \beta)d\mathbf{w} dX_{\text{missing}} \\\\ &amp;= \int p(\mathbf{y} \mid \mathbf{w}, X, \alpha, \beta) p(\mathbf{w}, X_{\text{missing}} \mid X_{\text{observed}}, \alpha, \beta) d\mathbf{w} dX_{\text{missing}} \\\\ &amp;= \mathbb{E}_{\mathbf{w}, X_{\text{missing}} \mid X_{\text{observed}}, \alpha, \beta} \left[ \mathcal{N}(\mathbf{y} \mid X \mathbf{w}, \beta^{-1}) \right] \end{align}\] <p>によって$p({\bf y} \mid X_{\rm observed}, \alpha, \beta)$を計算できるため，これを最大化する$(\alpha, \beta)$を求めればよいことになります．事後分布$p({\bf w}, X_{\rm missing} ~\mid X_{\rm observed}~~, \alpha, \beta)$に基づく${\bf w}, X_{\rm missing}$をサンプリングすることができるため，以下の手続きで$\alpha$, $\beta$を調整できます：</p> <p>(0) $\alpha_0$, $\beta_0$を適当に初期化 (1) $L(\alpha_t, \beta_t) = \mathbb{E} _ {\bf w}, X _ {\rm missing} ~\mid X _ {\rm observed}~~, \alpha_t, \beta_t} \left[ \mathcal{N}({\bf y} \mid X {\bf w}, \beta_t) \right]$を，事後分布に基づきサンプリングされた${\bf w}, X_{\rm missing}$から計算 (2) 数値微分により$\alpha_{t+1} = \alpha_t + \eta \dfrac{\partial L}{\partial \alpha}(\alpha_t, \beta_t)$のように更新 ここで$\eta$は適当に決め打ちする必要があります．またスケールの問題から，実際には対数尤度を計算する方がよいでしょう．</p> <h1 id="3-新しい入力に対する予測">3. 新しい入力に対する予測</h1> <p>新しい入力${\bf x}$に対して予測をしたいものとします．今回は${\bf x}$が欠損している可能性があるため，欠損している${\bf x} _ {\rm missing}$と${\bf x} _ {\rm observed}$に分けます．事後分布$p({\bf x} _ {\rm missing}, y \mid {\bf x} _ {\rm observed}, X_{\rm observed}, {\bf y}, \alpha, \beta)$を解析的に計算することはできないので，ギブスサンプリングによって分布を推定します． 先ほどと同様に，他の変数を条件づけたときの分布は</p> \[\mathbf{w} \mid * \sim \mathcal{N} \left( \left(X^T X+ \frac{\alpha}{\beta} I_m \right)^{-1} X^T \mathbf{y},~ \left(\beta X^T X + \alpha I_m \right)^{-1} \right) \\\\ x_{m} \mid * \sim \mathcal{N} \left( \frac{\beta y w_m - \sum_{m' \neq m} (\beta w_{m'} w_m + \Lambda_{m, m'} ) x_{m'} }{\beta w_m^2 + \Lambda_{m,m} },~ \frac{1}{\beta w_m^2 + \Lambda_{m,m}} \right) \\\\ y \mid * \sim \mathcal{N}(\mathbf{w}^T \mathbf{x}, \beta^{-1})\] <p>となっているため，事後分布$p({\bf x} _ {\rm missing}, y , {\bf w} \mid {\bf x} _ {\rm observed}, X_{\rm observed}, {\bf y}, \alpha, \beta)$をサンプリングによって推定できます．ここで関係ないはずの${\bf w}$が混入していることに違和感があるかもしれませんが，${\bf w}$の事後分布がサンプルを通じてしか求められず，$y$の分布が${\bf w}$を通じて求められる以上，${\bf w}$も含めてサンプルする必要があります． いま，${\bf w}$は訓練データ$X, {\bf y}$のみに依ることに注意します．そのため，${\bf w}$は依然サンプリングした結果を保存してそのまま使うことができます．求めたいのは${\bf x}$と$y$の分布なので，サンプリングした結果から平均や分散を計算すればよいでしょう．</p> <h1 id="実装">実装</h1> <p>先ほどの理論に基づき，説明変数の補間が可能なベイズ線形回帰のクラス<code class="language-plaintext highlighter-rouge">Bayesian_LR_with_intepolation</code>を実装しました．このクラスの便利なところは，欠損に対する前処理をすることなくデータを入れてしまえばよいという点です．また標準化や事前分布の設定もクラス内でいい感じにやってくれます． 主なメソッドは以下のようになっています： <code class="language-plaintext highlighter-rouge">fit</code>：訓練データを標準化し，ハイパーパラメータを調整する．その後，事後分布$p({\bf w}, X_{\rm missing} \mid *)$に従う${\bf w}$と$X_{\rm missing}$をサンプリングする．サンプルの平均と標準偏差(つまり事後分布に基づくの期待値と標準偏差)を計算できる． <code class="language-plaintext highlighter-rouge">predict</code>：与えられたテストデータに対する予測をする．訓練データとテストデータを使いサンプリングし，出力の期待値と標準偏差を返す．テストデータに欠損がある場合は，欠損値に対する予測もする．</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Bayesian_LR_with_interpolation</span><span class="p">:</span> <span class="c1"># 説明変数の補間が可能な，ベイズ線形回帰
</span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">beta</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span> <span class="c1"># alphaはパラメータwの事前分布のパラメータで，alphaが大きいほどwの分布は狭くなる．
</span>        <span class="n">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">beta</span> <span class="c1"># betaは誤差のオーダーの逆数
</span>    
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X_train_original</span><span class="p">,</span> <span class="n">y_train_original</span><span class="p">,</span> <span class="n">num_sample</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">fit_hyper_parameters</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">num_iteration</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">eta</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">):</span>
        <span class="n">dalpha</span> <span class="o">=</span> <span class="mf">0.01</span>
        <span class="n">dbeta</span> <span class="o">=</span> <span class="mf">0.01</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hyper_param_record</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">num_iteration</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span> <span class="c1"># ハイパーパラメータの軌跡を保存
</span>        <span class="n">self</span><span class="p">.</span><span class="nf">normalize_train</span><span class="p">(</span><span class="n">X_train_original</span><span class="p">,</span> <span class="n">y_train_original</span><span class="p">)</span> <span class="c1"># 標準化されたself.X_train, self.y_trainが生成される
</span>        <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">nan_list</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Note: There are no missing values in the explanatory variables.</span><span class="sh">"</span><span class="p">)</span>
        
        <span class="n">w_sample_mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span> <span class="c1"># サンプリングから計算したwの期待値を格納
</span>        <span class="n">w_sample_std</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span> <span class="c1"># サンプリングから計算したwの標準偏差を格納
</span>        <span class="n">X_sample_mean</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">X_train</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span> <span class="c1"># サンプリングから計算したXの期待値を格納，欠損値だけ更新
</span>        <span class="n">X_sample_std</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">X_train</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># サンプリングから計算したXの標準偏差を格納，欠損値だけ更新
</span>        <span class="n">X_sample</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">X_train</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span> <span class="c1"># 現時点でのサンプルの値，欠損値だけ更新する
</span>        <span class="n">self</span><span class="p">.</span><span class="n">w_samples</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">num_sample</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">X_train</span><span class="p">[</span><span class="mi">0</span><span class="p">])))</span> <span class="c1"># wに関するサンプリング結果をすべて集める，予測の際に必要
</span>        
        <span class="c1"># ハイパーパラメータを調整，勾配法により尤度を上げるようなハイパーパラメータを探す
</span>        <span class="k">if</span> <span class="n">fit_hyper_parameters</span><span class="p">:</span> <span class="c1"># ハイパーパラメータを調整する場合
</span>            <span class="n">likelihood</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">likelihood</span><span class="p">(</span><span class="n">num_sample</span><span class="p">)</span> <span class="c1"># 初期化
</span>            <span class="k">for</span> <span class="n">idx_iteration</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_iteration</span><span class="p">):</span>
                <span class="n">self</span><span class="p">.</span><span class="n">hyper_param_record</span><span class="p">[</span><span class="n">idx_iteration</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">self</span><span class="p">.</span><span class="n">alpha</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">beta</span><span class="p">])</span>
                <span class="n">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">+</span> <span class="n">dalpha</span>
                <span class="n">likelihood_new</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">likelihood</span><span class="p">(</span><span class="n">num_sample</span><span class="p">)</span>
                <span class="n">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">-</span> <span class="n">dalpha</span> <span class="o">+</span> <span class="n">eta</span><span class="o">*</span><span class="p">(</span><span class="n">likelihood_new</span> <span class="o">-</span> <span class="n">likelihood</span><span class="p">)</span> <span class="o">/</span> <span class="n">dalpha</span>
                <span class="n">likelihood</span> <span class="o">=</span> <span class="n">likelihood_new</span>
                <span class="n">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">+</span> <span class="n">dbeta</span>
                <span class="n">likelihood_new</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">likelihood</span><span class="p">(</span><span class="n">num_sample</span><span class="p">)</span> <span class="c1"># 新しい尤度を計算
</span>                <span class="n">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">-</span> <span class="n">dbeta</span> <span class="o">+</span> <span class="n">eta</span><span class="o">*</span><span class="p">(</span><span class="n">likelihood_new</span> <span class="o">-</span> <span class="n">likelihood</span><span class="p">)</span> <span class="o">/</span> <span class="n">dbeta</span>
                <span class="n">likelihood</span> <span class="o">=</span> <span class="n">likelihood_new</span>
                <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">iteraton = {0:} / {1:}: alpha = {2:.6f}, beta = {3:.6f}, L = {4:.6f}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">idx_iteration</span><span class="p">,</span> <span class="n">num_iteration</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">alpha</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">beta</span><span class="p">,</span> <span class="n">likelihood</span><span class="p">),</span> <span class="n">end</span> <span class="o">=</span> <span class="sh">"</span><span class="se">\r</span><span class="sh">"</span><span class="p">)</span>
        
        <span class="c1"># サンプリングによりwとXの統計量を計算
</span>        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_sample</span><span class="p">):</span>
            <span class="c1"># wのサンプリング
</span>            <span class="n">w_sample</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">sample_w</span><span class="p">(</span><span class="n">X_sample</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">w_samples</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">w_sample</span> <span class="c1"># predictionで使うため
</span>            <span class="n">w_sample_mean</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">update_mean</span><span class="p">(</span><span class="n">w_sample_mean</span><span class="p">,</span> <span class="n">w_sample</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
            <span class="n">w_sample_std</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">update_std</span><span class="p">(</span><span class="n">w_sample_std</span><span class="p">,</span> <span class="n">w_sample</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
            
            <span class="c1"># Xのサンプリング(欠損している箇所のみ)
</span>            <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">nan_list</span><span class="p">):</span>
                <span class="n">X_sample</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">sample_x_nm</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">w_sample</span><span class="p">,</span> <span class="n">X_sample</span><span class="p">[</span><span class="n">n</span><span class="p">],</span> <span class="n">self</span><span class="p">.</span><span class="n">y_train</span><span class="p">[</span><span class="n">n</span><span class="p">])</span>
                <span class="n">X_sample_mean</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">update_mean</span><span class="p">(</span><span class="n">X_sample_mean</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">],</span> <span class="n">X_sample</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">],</span> <span class="n">t</span><span class="p">)</span>
                <span class="n">X_sample_std</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">update_std</span><span class="p">(</span><span class="n">X_sample_std</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">],</span> <span class="n">X_sample</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">],</span> <span class="n">t</span><span class="p">)</span>
                
        <span class="c1"># 標準化を解除
</span>        <span class="n">X_sample_mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">tile</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">x_train_original_mean</span><span class="p">,</span> <span class="n">reps</span> <span class="o">=</span> <span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">X_train_original</span><span class="p">),</span> <span class="mi">1</span><span class="p">))</span> \
                            <span class="o">+</span> <span class="n">X_sample_mean</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">tile</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">x_train_original_std</span><span class="p">,</span> <span class="n">reps</span> <span class="o">=</span> <span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">X_train_original</span><span class="p">),</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">X_sample_std</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">tile</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">x_train_original_std</span><span class="p">,</span> <span class="n">reps</span> <span class="o">=</span> <span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">X_train_original</span><span class="p">),</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">X_sample_std</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Fitting has finished: </span><span class="sh">"</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">alpha = {:.6f}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">alpha</span><span class="p">))</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">beta = {:.6f}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">beta</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">w_sample_mean</span><span class="p">,</span> <span class="n">w_sample_std</span><span class="p">,</span> <span class="n">X_sample_mean</span><span class="p">,</span> <span class="n">X_sample_std</span>
    
    <span class="k">def</span> <span class="nf">normalize_train</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X_train_original</span><span class="p">,</span> <span class="n">y_train_original</span><span class="p">):</span> <span class="c1"># 入力を標準化しつつ，標準化を解除できるように値を保存
</span>        <span class="c1"># Xについての処理
</span>        <span class="n">self</span><span class="p">.</span><span class="n">x_train_original_mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">nanmean</span><span class="p">(</span><span class="n">X_train_original</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">x_train_original_std</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">nanstd</span><span class="p">(</span><span class="n">X_train_original</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">X_train</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_train_original</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nf">tile</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">x_train_original_mean</span><span class="p">,</span> <span class="n">reps</span> <span class="o">=</span> <span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">X_train_original</span><span class="p">),</span><span class="mi">1</span><span class="p">))</span> <span class="p">)</span> \
                            <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">tile</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">x_train_original_std</span><span class="p">,</span> <span class="n">reps</span> <span class="o">=</span> <span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">X_train_original</span><span class="p">),</span> <span class="mi">1</span><span class="p">))</span> <span class="c1"># 標準化
</span>        <span class="n">self</span><span class="p">.</span><span class="n">Lambda</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">inv</span><span class="p">(</span> <span class="n">np</span><span class="p">.</span><span class="nf">cov</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">X_train</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">isnan</span><span class="p">(</span><span class="n">X_train_original</span><span class="p">).</span><span class="nf">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">].</span><span class="n">T</span><span class="p">)</span> <span class="p">)</span><span class="c1"># 説明変数がどれも欠損していないデータ行から計算した共分散行列の逆行列
</span>        <span class="n">self</span><span class="p">.</span><span class="n">nan_list</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">isnan</span><span class="p">(</span><span class="n">X_train_original</span><span class="p">))))</span> <span class="c1"># もとの入力データでnullの場所を格納
</span>        <span class="n">self</span><span class="p">.</span><span class="n">X_train</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">isnan</span><span class="p">(</span><span class="n">X_train_original</span><span class="p">)]</span> <span class="o">=</span> <span class="mf">0.0</span> <span class="c1"># 標準化しているため0で埋めてok
</span>        
        <span class="c1"># yについての処理
</span>        <span class="n">self</span><span class="p">.</span><span class="n">y_train_original_mean</span> <span class="o">=</span> <span class="n">y_train_original</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">y_train_original_std</span> <span class="o">=</span> <span class="n">y_train_original</span><span class="p">.</span><span class="nf">std</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">y_train</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_train_original</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">y_train_original_mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">y_train_original_std</span>
    
    <span class="k">def</span> <span class="nf">likelihood</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">num_sample</span><span class="p">):</span>
        <span class="n">L</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="n">X_sample</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">X_train</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_sample</span><span class="p">):</span>
            <span class="n">w_sample</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">sample_w</span><span class="p">(</span><span class="n">X_sample</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">nan_list</span><span class="p">):</span>
                <span class="n">X_sample</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">sample_x_nm</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">w_sample</span><span class="p">,</span> <span class="n">X_sample</span><span class="p">[</span><span class="n">n</span><span class="p">],</span> <span class="n">self</span><span class="p">.</span><span class="n">y_train</span><span class="p">[</span><span class="n">n</span><span class="p">])</span>
            <span class="n">L</span> <span class="o">+=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">*</span> <span class="p">((</span><span class="n">self</span><span class="p">.</span><span class="n">y_train</span> <span class="o">-</span> <span class="n">X_sample</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">w_sample</span><span class="p">))</span><span class="o">**</span><span class="mf">2.0</span><span class="p">).</span><span class="nf">sum</span><span class="p">()</span> <span class="p">)</span>
        <span class="c1"># 訓練データ数が増えるにつれてlog_likelihoodは線形に増える(はず)なので規格化．これをしないとデータ数を変えるたびにetaを変更する必要がありそう
</span>        <span class="n">L</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">L</span><span class="p">)</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">y_train</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">beta</span><span class="p">)</span> 
        <span class="k">return</span> <span class="n">L</span>
    
    <span class="k">def</span> <span class="nf">update_mean</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x_mean</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="nf">return </span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">t</span><span class="o">+</span><span class="mf">1.0</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span><span class="n">t</span><span class="o">*</span><span class="n">x_mean</span> <span class="o">+</span> <span class="n">x</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">update_std</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x_std</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span> <span class="p">(</span><span class="n">t</span><span class="o">/</span><span class="p">(</span><span class="n">t</span><span class="o">+</span><span class="mf">1.0</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span><span class="n">x_std</span><span class="o">**</span><span class="mf">2.0</span> <span class="o">+</span> <span class="p">(</span><span class="n">x_std</span><span class="o">-</span><span class="n">x</span><span class="p">)</span><span class="o">**</span><span class="mf">2.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">t</span><span class="o">+</span><span class="mf">1.0</span><span class="p">)</span> <span class="p">)</span> <span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">sample_w</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span> <span class="c1"># 他を条件づけたときのwをサンプル
</span>        <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">y_train</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">alpha</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">beta</span>
        <span class="n">w_mu</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">inv</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">alpha</span><span class="o">/</span><span class="n">beta</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">]))).</span><span class="nf">dot</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">T</span><span class="p">).</span><span class="nf">dot</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="c1"># 他の変数を条件づけたときの分布の期待値n
</span>        <span class="n">w_sigma</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">inv</span><span class="p">(</span><span class="n">beta</span> <span class="o">*</span> <span class="n">X</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">alpha</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">])))</span> <span class="c1"># 標準偏差
</span>        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">multivariate_normal</span><span class="p">(</span><span class="n">w_mu</span><span class="p">,</span> <span class="n">w_sigma</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">sample_x_nm</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">x_n</span><span class="p">,</span> <span class="n">y_n</span><span class="p">):</span> <span class="c1"># 他を条件づけたときのx_nm(欠損値)をサンプル
</span>        <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">alpha</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">beta</span>
        <span class="n">x_nm_mu</span> <span class="o">=</span> <span class="n">x_n</span><span class="p">[</span><span class="n">m</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">beta</span><span class="o">*</span><span class="n">y_n</span><span class="o">*</span><span class="n">w</span><span class="p">[</span><span class="n">m</span><span class="p">]</span> <span class="o">-</span> <span class="p">(</span><span class="n">beta</span><span class="o">*</span><span class="n">w</span><span class="o">*</span><span class="n">w</span><span class="p">[</span><span class="n">m</span><span class="p">]</span><span class="o">+</span><span class="n">self</span><span class="p">.</span><span class="n">Lambda</span><span class="p">[</span><span class="n">m</span><span class="p">]).</span><span class="nf">dot</span><span class="p">(</span><span class="n">x_n</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">beta</span><span class="o">*</span><span class="n">w</span><span class="p">[</span><span class="n">m</span><span class="p">]</span><span class="o">*</span><span class="n">w</span><span class="p">[</span><span class="n">m</span><span class="p">]</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">Lambda</span><span class="p">[</span><span class="n">m</span><span class="p">,</span><span class="n">m</span><span class="p">])</span>
        <span class="n">x_nm_sigma</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">beta</span><span class="o">*</span><span class="n">w</span><span class="p">[</span><span class="n">m</span><span class="p">]</span><span class="o">*</span><span class="n">w</span><span class="p">[</span><span class="n">m</span><span class="p">]</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">Lambda</span><span class="p">[</span><span class="n">m</span><span class="p">,</span><span class="n">m</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">x_nm_mu</span><span class="p">,</span> <span class="n">x_nm_sigma</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">sample_y</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span> <span class="c1"># 他を条件づけたときの出力yをサンプル
</span>        <span class="n">y_mu</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
        <span class="n">y_sigma</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span><span class="o">/</span><span class="n">self</span><span class="p">.</span><span class="n">beta</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">y_mu</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">multivariate_normal</span><span class="p">(</span><span class="n">y_mu</span><span class="p">,</span> <span class="n">y_sigma</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X_test_original</span><span class="p">,</span> <span class="n">num_sample</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">):</span> <span class="c1"># 欠損がある可能性のある入力から予測
</span>        <span class="n">X_test</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_test_original</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nf">tile</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">x_train_original_mean</span><span class="p">,</span> <span class="n">reps</span> <span class="o">=</span> <span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">X_test_original</span><span class="p">),</span><span class="mi">1</span><span class="p">))</span> <span class="p">)</span> \
                            <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">tile</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">x_train_original_std</span><span class="p">,</span> <span class="n">reps</span> <span class="o">=</span> <span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">X_test_original</span><span class="p">),</span> <span class="mi">1</span><span class="p">))</span> <span class="c1"># 標準化
</span>        <span class="n">nan_list</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">isnan</span><span class="p">(</span><span class="n">X_test_original</span><span class="p">))))</span> <span class="c1"># もとの入力データでnullの場所を格納
</span>        <span class="n">X_test</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">isnan</span><span class="p">(</span><span class="n">X_test_original</span><span class="p">)]</span> <span class="o">=</span> <span class="mf">0.0</span> <span class="c1"># 標準化しているため0で埋めてok
</span>        
        <span class="n">X_test_sample_mean</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span> <span class="c1"># サンプリングから計算したXの期待値を格納，欠損値だけ更新
</span>        <span class="n">X_test_sample_std</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">X_test</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># サンプリングから計算したXの標準偏差を格納，欠損値だけ更新
</span>        <span class="n">X_test_sample</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span> <span class="c1"># 現時点でのサンプルの値，欠損値だけ更新する
</span>        
        <span class="n">y_test_sample_mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">X_test_original</span><span class="p">))</span>
        <span class="n">y_test_sample_std</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">X_test_original</span><span class="p">))</span>
        
        <span class="c1"># サンプリングによりyとXの統計量を計算
</span>        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_sample</span><span class="p">):</span>
            <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">prediction: t = {0:} / {1:}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">num_sample</span><span class="p">),</span> <span class="n">end</span> <span class="o">=</span> <span class="sh">"</span><span class="se">\r</span><span class="sh">"</span><span class="p">)</span>
            <span class="c1"># wのサンプル(前の結果を流用)
</span>            <span class="n">w_sample</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">w_samples</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
            <span class="c1"># yのサンプル
</span>            <span class="n">y_test_sample</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">sample_y</span><span class="p">(</span><span class="n">X_test_sample</span><span class="p">,</span> <span class="n">w_sample</span><span class="p">)</span>
            <span class="n">y_test_sample_mean</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">update_mean</span><span class="p">(</span><span class="n">y_test_sample_mean</span><span class="p">,</span> <span class="n">y_test_sample</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
            <span class="n">y_test_sample_std</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">update_std</span><span class="p">(</span><span class="n">y_test_sample_std</span><span class="p">,</span> <span class="n">y_test_sample</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
            <span class="c1"># Xのサンプル           
</span>            <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">nan_list</span><span class="p">):</span>
                <span class="n">X_test_sample</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">sample_x_nm</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">w_sample</span><span class="p">,</span> <span class="n">X_test_sample</span><span class="p">[</span><span class="n">n</span><span class="p">],</span> <span class="n">y_test_sample</span><span class="p">[</span><span class="n">n</span><span class="p">])</span>
                <span class="n">X_test_sample_mean</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">update_mean</span><span class="p">(</span><span class="n">X_test_sample_mean</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">],</span> <span class="n">X_test_sample</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">],</span> <span class="n">t</span><span class="p">)</span>
                <span class="n">X_test_sample_std</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">update_std</span><span class="p">(</span><span class="n">X_test_sample_std</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">],</span> <span class="n">X_test_sample</span><span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">],</span> <span class="n">t</span><span class="p">)</span>
        <span class="c1"># 標準化を解除
</span>        <span class="n">y_test_sample_mean</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">y_train_original_mean</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">y_train_original_std</span> <span class="o">*</span> <span class="n">y_test_sample_mean</span>
        <span class="n">y_test_sample_std</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">y_train_original_std</span> <span class="o">*</span> <span class="n">y_test_sample_std</span>
        <span class="n">X_test_sample_mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">tile</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">x_train_original_mean</span><span class="p">,</span> <span class="n">reps</span> <span class="o">=</span> <span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span> <span class="mi">1</span><span class="p">))</span> \
                            <span class="o">+</span> <span class="n">X_test_sample_mean</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">tile</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">x_train_original_std</span><span class="p">,</span> <span class="n">reps</span> <span class="o">=</span> <span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">X_test_sample_std</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">tile</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">x_train_original_std</span><span class="p">,</span> <span class="n">reps</span> <span class="o">=</span> <span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">X_test_sample_std</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Predictions has finished.</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y_test_sample_mean</span><span class="p">,</span> <span class="n">y_test_sample_std</span><span class="p">,</span> <span class="n">X_test_sample_mean</span><span class="p">,</span> <span class="n">X_test_sample_std</span>
    
</code></pre></div></div> <h1 id="数値実験">数値実験</h1> <p>実装がうまくいっているか確認するために，トイモデルを作成してみました．実装は以下のリンクにあります． https://github.com/yotapoon/Bayesian_LR_with_interpolation</p> <h2 id="設定">設定</h2> <p>説明変数は$(x_1, x_2)$とし，目的変数を$y$とします．これらは</p> <pre><code class="language-math">x_1 \sim \mathcal{N}(2.0, 0.4) \\
x_2 \mid x_1 \sim \mathcal{N}(-2.0 x_1 + 0.5, 0.8) \\
y \mid x_1,x_2 \sim \mathcal{N}(3.0 x_1 + 1.5x_2, 0.8)
</code></pre> <p>のような関係にあるものとし，訓練データとして$N = 100$点を生成します． ここではMCARの状況を考えます．つまり，欠損の発生が他の変数に依存しないような扱いやすい状況です．また訓練データの入力値$X$の各要素には確率$p = 0.3$で欠損が生じるものとします．$X$は以下のような形状です：</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[ 1.58535427         nan]
 [ 2.10601671 -4.55318535]
 [ 1.80926526 -3.5396434 ]
...
</code></pre></div></div> <p>上のようなデータに対して，以下の四つの方法により<code class="language-plaintext highlighter-rouge">fit</code>します： ① 欠損のないデータ(complete data)を用いて回帰． ② $x_2$をその平均値により補完． ③ $x_2$が欠損している行をそのまま削除． ④ ベイズ線形回帰により補完．</p> <p>また同様のテストデータを$N’ = 1000$点生成し，テスト誤差を計算することでそれぞれの性能を評価します．ただし，各手法を比較しやすくするためにテストデータに欠損はないものとします． 期待される結果は以下の通りです： ・欠損のないデータによる回帰①は最も良い性能を与えてほしい． ・平均値による補完②はあまりよくないと聞くので，それほど精度がでないはず ・欠損している行をそのまま削除する方法③は情報量が減るためそこまでいい結果にはならないはず ・ベイズ線形回帰④をすると欠損のないデータによる回帰①より精度は出ないはずだが，適当な補完②や行の削除③よりは精度は出てほしい</p> <h2 id="結果と考察">結果と考察</h2> <p>数値実験の結果を以下に示します．確認のため，今回実装したクラスを使わない最尤推定の結果も示しています．<code class="language-plaintext highlighter-rouge">y_error</code>は$y$に関するテスト誤差の平均値，<code class="language-plaintext highlighter-rouge">X_error</code>は$X$に関する訓練誤差の平均値です．</p> <p>① 欠損のないデータ(complete data)を用いて回帰．</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>=====最尤推定=====
y_error = 0.6841894767175021

=====ベイズ線形回帰=====
Note: There are no missing values in the explanatory variables.
alpha = 1.482402
beta = 2.115292
y_error = 0.6814939439707199
</code></pre></div></div> <p>② $x_2$をその平均値により補完．</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>=====最尤推定=====
y_error = 0.9199739418129449

=====ベイズ線形回帰=====
Note: There are no missing values in the explanatory variables.
alpha = 4.767952
beta = 1.785241
X_error = 0.03779597748274369, 0.5054058111998435
y_error = 0.9434735826603966
</code></pre></div></div> <p>③ $x_2$が欠損している行をそのまま削除．</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>=====最尤推定=====
y_error = 0.7259293059445424

=====ベイズ線形回帰=====
Note: There are no missing values in the explanatory variables.
alpha = 1.721914
beta = 3.476663
y_error = 0.7766452769398361
</code></pre></div></div> <p>④ ベイズ線形回帰により補完．</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>=====ベイズ線形回帰=====
alpha = 2.567708
beta = 4.346280
X_error = 0.024276115858132833, 0.2436076829842362
y_error = 0.7147843731641442
</code></pre></div></div> <p>最尤推定とベイズ線形回帰の$y$に関する誤差が割と整合しているので，結果は正しそうです． また<code class="language-plaintext highlighter-rouge">y_error</code>を比較すると，平均で補完 &gt; そのまま削除 &gt; ベイズ線形回帰 &gt; complete dataとなっているため，期待された結果が得られています．さらに<code class="language-plaintext highlighter-rouge">X_error</code>をみると，ベイズ線形回帰は平均値による補完と比較して優れていることがわかります． 今回は欠損の生じていないテストデータに対する予測により評価しましたが，本手法は欠損が生じているデータに対してもある程度の精度で予測が可能です．これは他の手法と比べると大きなアドバンテージです．</p> <p>一方で，生成したデータによってはベイズ線形回帰の優位性がそれほど明らかでなかった場合がありました．特にデータ数が多かったりノイズが小さかったりするときには，欠損している行を削除してしまってもよい結果が得られます．そのため，本手法が効果を発揮するのは「入出力がある程度の相関を持ちつつ，ノイズも大きいような場合」であるといえそうです．</p> <h1 id="まとめ">まとめ</h1> <p>今回は欠損のある入力をベイズ的に補完しつつ，欠損値の推定，ハイパーパラメータの調整，新しい入力に対する予測を行うクラスを実装しました．数値実験から，実装が正しそうであることとベイズ線形回帰の効果を確認しました．本手法のメリットとしては， ・情報量増加による予測精度の向上 ・訓練データへの前処理の自動化 ・欠損のあるデータに対する予測能力 があります．特に最後の点に関しては，その他の手法に比べ非常に大きな利点であるといえます．</p> <p>一方で，デメリットとしては ・導出，実装が非常に面倒であること ・サンプリングのために計算量が増加すること が挙げられます．前者に関しては，欠損値を埋めたいという強いモチベーションがない限り，割に合わないというのが正直な感想です(導出・実装するのに丸二日かかった)．後者に関しては，事後分布を解析的に書けないためどうしても避けられないと考えます(もしかしたらできるのかもしれませんが．．．たかが線形回帰と思っていましたが，意外と複雑になるようです)．</p> <p>今回は最も扱いやすいMCARにフォーカスしましたが，MARの場合は欠損が生じる確率をモデル化することにより同様の解析が可能になります．きちんとは読んでいませんが， https://bookdown.org/marklhc/notes_bookdown/missing-data.html がそのような内容になっていると思います．時間があれば，MARの場合にも同様の実装をするつもりです．</p> <p>また，平均値による補完と欠損値の削除がそれぞれ有利になるのはどのような状況なのかが少し気になりました．データ数やノイズの大きさ，変数間の相関に依存すると思いますが，理論的な解析もできるのかもしれません．</p>]]></content><author><name></name></author><category term="sample-posts"/><summary type="html"><![CDATA[Statistical Machine Learning with Bayes Imputation in Python]]></summary></entry><entry><title type="html">Research on M&amp;amp;A in Pharmaceutical industry - Name Matching Codes</title><link href="https://idsts2670.github.io/blog/2023/name-matching-pharma/" rel="alternate" type="text/html" title="Research on M&amp;amp;A in Pharmaceutical industry - Name Matching Codes"/><published>2023-10-30T00:00:00-04:00</published><updated>2023-10-30T00:00:00-04:00</updated><id>https://idsts2670.github.io/blog/2023/name-matching-pharma</id><content type="html" xml:base="https://idsts2670.github.io/blog/2023/name-matching-pharma/"><![CDATA[<div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/name_matching_pharma.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div>]]></content><author><name></name></author><category term="sample-posts"/><category term="jupyter"/><summary type="html"><![CDATA[Introduction of Amex competition EDA jupyternotebook]]></summary></entry><entry><title type="html">a post with jupyter notebook</title><link href="https://idsts2670.github.io/blog/2023/jupyter-notebook/" rel="alternate" type="text/html" title="a post with jupyter notebook"/><published>2023-07-04T08:57:00-04:00</published><updated>2023-07-04T08:57:00-04:00</updated><id>https://idsts2670.github.io/blog/2023/jupyter-notebook</id><content type="html" xml:base="https://idsts2670.github.io/blog/2023/jupyter-notebook/"><![CDATA[<p>To include a jupyter notebook in a post, you can use the following code:</p> <div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{::nomarkdown}
{% assign jupyter_path = "assets/jupyter/blog.ipynb" | relative_url %}
{% capture notebook_exists %}{% file_exists assets/jupyter/blog.ipynb %}{% endcapture %}
{% if notebook_exists == "true" %}
    {% jupyter_notebook jupyter_path %}
{% else %}
    <span class="nt">&lt;p&gt;</span>Sorry, the notebook you are looking for does not exist.<span class="nt">&lt;/p&gt;</span>
{% endif %}
{:/nomarkdown}
</code></pre></div></div> <p>Let’s break it down: this is possible thanks to <a href="https://github.com/red-data-tools/jekyll-jupyter-notebook">Jekyll Jupyter Notebook plugin</a> that allows you to embed jupyter notebooks in your posts. It basically calls <a href="https://nbconvert.readthedocs.io/en/latest/usage.html#convert-html"><code class="language-plaintext highlighter-rouge">jupyter nbconvert --to html</code></a> to convert the notebook to an html page and then includes it in the post. Since <a href="https://jekyllrb.com/docs/configuration/markdown/">Kramdown</a> is the default Markdown renderer for Jekyll, we need to surround the call to the plugin with the <a href="https://kramdown.gettalong.org/syntax.html#extensions">::nomarkdown</a> tag so that it stops processing this part with Kramdown and outputs the content as-is.</p> <p>The plugin takes as input the path to the notebook, but it assumes the file exists. If you want to check if the file exists before calling the plugin, you can use the <code class="language-plaintext highlighter-rouge">file_exists</code> filter. This avoids getting a 404 error from the plugin and ending up displaying the main page inside of it instead. If the file does not exist, you can output a message to the user. The code displayed above outputs the following:</p> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/blog.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div> <p>Note that the jupyter notebook supports both light and dark themes.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="jupyter"/><summary type="html"><![CDATA[an example of a blog post with jupyter notebook]]></summary></entry><entry><title type="html">a post with custom blockquotes</title><link href="https://idsts2670.github.io/blog/2023/custom-blockquotes/" rel="alternate" type="text/html" title="a post with custom blockquotes"/><published>2023-05-12T15:53:00-04:00</published><updated>2023-05-12T15:53:00-04:00</updated><id>https://idsts2670.github.io/blog/2023/custom-blockquotes</id><content type="html" xml:base="https://idsts2670.github.io/blog/2023/custom-blockquotes/"><![CDATA[<p>This post shows how to add custom styles for blockquotes. Based on <a href="https://github.com/sighingnow/jekyll-gitbook">jekyll-gitbook</a> implementation.</p> <p>We decided to support the same custom blockquotes as in <a href="https://sighingnow.github.io/jekyll-gitbook/jekyll/2022-06-30-tips_warnings_dangers.html">jekyll-gitbook</a>, which are also found in a lot of other sites’ styles. The styles definitions can be found on the <a href="https://github.com/alshedivat/al-folio/blob/master/_sass/_base.scss">_base.scss</a> file, more specifically:</p> <div class="language-scss highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cm">/* Tips, warnings, and dangers */</span>
<span class="nc">.post</span> <span class="nc">.post-content</span> <span class="nt">blockquote</span> <span class="p">{</span>
    <span class="k">&amp;</span><span class="nc">.block-tip</span> <span class="p">{</span>
    <span class="nl">border-color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-tip-block</span><span class="p">);</span>
    <span class="nl">background-color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-tip-block-bg</span><span class="p">);</span>

    <span class="nt">p</span> <span class="p">{</span>
      <span class="nl">color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-tip-block-text</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="nt">h1</span><span class="o">,</span> <span class="nt">h2</span><span class="o">,</span> <span class="nt">h3</span><span class="o">,</span> <span class="nt">h4</span><span class="o">,</span> <span class="nt">h5</span><span class="o">,</span> <span class="nt">h6</span> <span class="p">{</span>
      <span class="nl">color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-tip-block-title</span><span class="p">);</span>
    <span class="p">}</span>
  <span class="p">}</span>

  <span class="k">&amp;</span><span class="nc">.block-warning</span> <span class="p">{</span>
    <span class="nl">border-color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-warning-block</span><span class="p">);</span>
    <span class="nl">background-color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-warning-block-bg</span><span class="p">);</span>

    <span class="nt">p</span> <span class="p">{</span>
      <span class="nl">color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-warning-block-text</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="nt">h1</span><span class="o">,</span> <span class="nt">h2</span><span class="o">,</span> <span class="nt">h3</span><span class="o">,</span> <span class="nt">h4</span><span class="o">,</span> <span class="nt">h5</span><span class="o">,</span> <span class="nt">h6</span> <span class="p">{</span>
      <span class="nl">color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-warning-block-title</span><span class="p">);</span>
    <span class="p">}</span>
  <span class="p">}</span>

  <span class="k">&amp;</span><span class="nc">.block-danger</span> <span class="p">{</span>
    <span class="nl">border-color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-danger-block</span><span class="p">);</span>
    <span class="nl">background-color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-danger-block-bg</span><span class="p">);</span>

    <span class="nt">p</span> <span class="p">{</span>
      <span class="nl">color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-danger-block-text</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="nt">h1</span><span class="o">,</span> <span class="nt">h2</span><span class="o">,</span> <span class="nt">h3</span><span class="o">,</span> <span class="nt">h4</span><span class="o">,</span> <span class="nt">h5</span><span class="o">,</span> <span class="nt">h6</span> <span class="p">{</span>
      <span class="nl">color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-danger-block-title</span><span class="p">);</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <p>A regular blockquote can be used as following:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gt">&gt; This is a regular blockquote</span>
<span class="gt">&gt; and it can be used as usual</span>
</code></pre></div></div> <blockquote> <p>This is a regular blockquote and it can be used as usual</p> </blockquote> <p>These custom styles can be used by adding the specific class to the blockquote, as follows:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gt">&gt; ##### TIP</span>
<span class="gt">&gt;</span>
<span class="gt">&gt; A tip can be used when you want to give advice</span>
<span class="gt">&gt; related to a certain content.</span>
{: .block-tip }
</code></pre></div></div> <blockquote class="block-tip"> <h5 id="tip">TIP</h5> <p>A tip can be used when you want to give advice related to a certain content.</p> </blockquote> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gt">&gt; ##### WARNING</span>
<span class="gt">&gt;</span>
<span class="gt">&gt; This is a warning, and thus should</span>
<span class="gt">&gt; be used when you want to warn the user</span>
{: .block-warning }
</code></pre></div></div> <blockquote class="block-warning"> <h5 id="warning">WARNING</h5> <p>This is a warning, and thus should be used when you want to warn the user</p> </blockquote> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gt">&gt; ##### DANGER</span>
<span class="gt">&gt;</span>
<span class="gt">&gt; This is a danger zone, and thus should</span>
<span class="gt">&gt; be used carefully</span>
{: .block-danger }
</code></pre></div></div> <blockquote class="block-danger"> <h5 id="danger">DANGER</h5> <p>This is a danger zone, and thus should be used carefully</p> </blockquote>]]></content><author><name></name></author><category term="sample-posts"/><summary type="html"><![CDATA[an example of a blog post with custom blockquotes]]></summary></entry><entry><title type="html">a post with table of contents on a sidebar</title><link href="https://idsts2670.github.io/blog/2023/sidebar-table-of-contents/" rel="alternate" type="text/html" title="a post with table of contents on a sidebar"/><published>2023-04-25T10:14:00-04:00</published><updated>2023-04-25T10:14:00-04:00</updated><id>https://idsts2670.github.io/blog/2023/sidebar-table-of-contents</id><content type="html" xml:base="https://idsts2670.github.io/blog/2023/sidebar-table-of-contents/"><![CDATA[<p>This post shows how to add a table of contents as a sidebar.</p> <h2 id="adding-a-table-of-contents">Adding a Table of Contents</h2> <p>To add a table of contents to a post as a sidebar, simply add</p> <div class="language-yml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">toc</span><span class="pi">:</span>
  <span class="na">sidebar</span><span class="pi">:</span> <span class="s">left</span>
</code></pre></div></div> <p>to the front matter of the post. The table of contents will be automatically generated from the headings in the post. If you wish to display the sidebar to the right, simply change <code class="language-plaintext highlighter-rouge">left</code> to <code class="language-plaintext highlighter-rouge">right</code>.</p> <h3 id="example-of-sub-heading-1">Example of Sub-Heading 1</h3> <p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p> <h3 id="example-of-another-sub-heading-1">Example of another Sub-Heading 1</h3> <p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p> <h2 data-toc-text="Customizing" id="customizing-your-table-of-contents">Customizing Your Table of Contents</h2> <p>If you want to learn more about how to customize the table of contents of your sidebar, you can check the <a href="https://afeld.github.io/bootstrap-toc/">bootstrap-toc</a> documentation. Notice that you can even customize the text of the heading that will be displayed on the sidebar.</p> <h3 id="example-of-sub-heading-2">Example of Sub-Heading 2</h3> <p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p> <h3 id="example-of-another-sub-heading-2">Example of another Sub-Heading 2</h3> <p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="toc"/><category term="sidebar"/><summary type="html"><![CDATA[an example of a blog post with table of contents on a sidebar]]></summary></entry></feed>